{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "from torch.nn import Softmax\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from llmtosql.model import WikiSQLModel\n",
    "from llmtosql.trainer import Trainer\n",
    "from llmtosql.dataloader import WikiSQLDataset\n",
    "from llmtosql.utils.utils import plot_history, plot_history_base, load_model, load_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "path = 'model_output'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-10 16:39:04 [info     ] Using cross attention mechanism\n",
      "2023-05-10 16:39:04 [info     ] 3 heads model -- ['SELECT', 'AGG', 'CONDS']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = WikiSQLModel(base_model_type='bert-base-uncased', attention_type='cross')\n",
    "model = load_model(model, 'model_output/model.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:14:49 [info     ] Tokenizing dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8421/8421 [00:07<00:00, 1171.23it/s]\n"
     ]
    }
   ],
   "source": [
    "test_set = WikiSQLDataset(type='dev', model=model)\n",
    "test_loader = DataLoader(test_set, batch_size=32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/264 [00:37<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]] [[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1]] [['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'What', '', '', '', '', '', '', ''], ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'What', '', '', '', '', '', '', '']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sel = []\n",
    "agg = []\n",
    "conds = []\n",
    "with tqdm(test_loader, unit='batch') as tepoch:\n",
    "    for data in tepoch:\n",
    "        questions = (data['input'][0])\n",
    "        inputs, _ = model.unpack(data, device)\n",
    "        outputs = model(inputs)\n",
    "        predictions = model.predict(outputs)\n",
    "        sel.extend(predictions[0].tolist())\n",
    "        agg.extend(predictions[1].tolist())\n",
    "        for idx, cond in enumerate(predictions[2]):\n",
    "            if len(cond.shape) == 1:\n",
    "                cond = cond.unsqueeze(1)\n",
    "            if idx == 0:\n",
    "                max_num_conditions = torch.max(cond).item()\n",
    "                # print(max_num_conditions)\n",
    "                if max_num_conditions == 0:\n",
    "                    cond_1 = cond_2, cond_3 = [[None]], [[None]], [[None]]\n",
    "                    break\n",
    "            elif idx == 1:\n",
    "                cond_1 = cond.T.tolist()\n",
    "                cond_1 = cond_1[:max_num_conditions]\n",
    "            elif idx == 2:\n",
    "                cond = cond - 1\n",
    "                cond_2 = cond.T.tolist()\n",
    "                cond_2 = cond_2[:max_num_conditions]\n",
    "            elif idx == 3:\n",
    "                outer_list = []\n",
    "                # print(torch.transpose(predictions[2][3].T, 1, 2).tolist())\n",
    "                for condition in torch.transpose(predictions[2][3].T, 1, 2).tolist():\n",
    "                    batch_list = [WikiSQLDataset._digitize(' '.join((q.split())[cond_range[0]:cond_range[0] + cond_range[1]])) for cond_range, q in zip(condition, questions)]\n",
    "                    # for batch in condition:\n",
    "                    #     word_list = model.tokenizer.convert_ids_to_tokens(batch, skip_special_tokens=True)\n",
    "                    #     batch_list.append(' '.join(word_list))\n",
    "                    outer_list.append(batch_list)\n",
    "                cond_3 = outer_list\n",
    "                cond_3 = cond_3[:max_num_conditions]\n",
    "        print(cond_1, cond_2, cond_3)\n",
    "        all_conds = []\n",
    "        for c1, c2, c3 in zip(cond_1, cond_2, cond_3):\n",
    "            inner_all_conds = []\n",
    "            for b1, b2, b3 in zip(c1, c2, c3):\n",
    "                if b2 == -1:\n",
    "                    b1, b2, b3 = None, None, None\n",
    "                inner_all_conds.append((b1, b2, b3))\n",
    "            all_conds.append(inner_all_conds)\n",
    "        conds.extend([list(x) for x in zip(*all_conds)])\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "final = []\n",
    "for s, a, c in zip(sel, agg, conds):\n",
    "    solution = {\n",
    "        \"query\": {\n",
    "            \"sel\":s,\n",
    "            \"agg\":a\n",
    "        }\n",
    "    }\n",
    "    if all([all([x is None for x in cond]) for cond in c]):\n",
    "        c = None\n",
    "    if c is not None:\n",
    "        solution[\"query\"][\"conds\"] = [list(x) for x in c]\n",
    "    final.append(solution)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'query': {'sel': 3, 'agg': 0}},\n {'query': {'sel': 1, 'agg': 3}},\n {'query': {'sel': 0, 'agg': 0}},\n {'query': {'sel': 0, 'agg': 0}},\n {'query': {'sel': 0, 'agg': 0}},\n {'query': {'sel': 0, 'agg': 0}},\n {'query': {'sel': 5, 'agg': 0}},\n {'query': {'sel': 1, 'agg': 3}},\n {'query': {'sel': 3, 'agg': 3}},\n {'query': {'sel': 2, 'agg': 0}},\n {'query': {'sel': 5, 'agg': 0}},\n {'query': {'sel': 2, 'agg': 0}},\n {'query': {'sel': 4, 'agg': 0}},\n {'query': {'sel': 0, 'agg': 0}},\n {'query': {'sel': 3, 'agg': 0}},\n {'query': {'sel': 0, 'agg': 0}},\n {'query': {'sel': 3, 'agg': 0}},\n {'query': {'sel': 3, 'agg': 0}},\n {'query': {'sel': 1, 'agg': 3}},\n {'query': {'sel': 5, 'agg': 0}},\n {'query': {'sel': 7, 'agg': 0}},\n {'query': {'sel': 5, 'agg': 2}},\n {'query': {'sel': 4, 'agg': 0}},\n {'query': {'sel': 4, 'agg': 0}},\n {'query': {'sel': 0, 'agg': 0, 'conds': [[0, 2, 'What'], [0, 2, 'What']]}},\n {'query': {'sel': 4, 'agg': 0}},\n {'query': {'sel': 4, 'agg': 3}},\n {'query': {'sel': 0, 'agg': 0}},\n {'query': {'sel': 5, 'agg': 0}},\n {'query': {'sel': 5, 'agg': 0}},\n {'query': {'sel': 1, 'agg': 3}},\n {'query': {'sel': 4, 'agg': 0}}]"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_file = 'model_output/test_results.jsonl'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(test_file, 'w+') as f:\n",
    "    for line in final:\n",
    "        json.dump(line, f)\n",
    "        f.write('\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
