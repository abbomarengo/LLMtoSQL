{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from llmtosql.model import WikiSQLModel\n",
    "from llmtosql.trainer import Trainer\n",
    "from llmtosql.dataloader import WikiSQLDataset\n",
    "from llmtosql.utils.utils import plot_history, load_model, load_history\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-08 19:43:40 [info     ] Using cross attention mechanism\n",
      "2023-05-08 19:43:40 [info     ] 1 heads model -- ['CONDS']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = WikiSQLModel(base_model_type='bert-base-uncased', heads=(False, False, True))\n",
    "# try:\n",
    "#     model = load_model(model, 'model_output/model.pth')\n",
    "# except Exception as e:\n",
    "#     print(f'No model loaded - {e}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-08 19:43:43 [info     ] Tokenizing dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8421/8421 [00:06<00:00, 1229.86it/s]\n"
     ]
    }
   ],
   "source": [
    "train_set =  WikiSQLDataset(type='dev', model=model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "4"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.maxcondsLength"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# val_set = WikiSQLDataset(type='dev')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "8421"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set.data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# datasets = (train_set, val_set)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "config = {\n",
    "    'seed': 32,\n",
    "    'scheduler': None,\n",
    "    'optimizer': 'adam',\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 0.0,\n",
    "    'lr': 0.0005,\n",
    "    'criterion': 'custom',\n",
    "    'metric': 'accuracy',\n",
    "    'model_dir': 'model_output',\n",
    "    'backend': 'smddp'\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-08 19:43:49 [info     ] Config inputs.                 config={'seed': 32, 'scheduler': None, 'optimizer': 'adam', 'momentum': 0.9, 'weight_decay': 0.0, 'lr': 0.0005, 'criterion': 'custom', 'metric': 'accuracy', 'model_dir': 'model_output', 'backend': 'smddp'}\n",
      "2023-05-08 19:43:49 [info     ] Loading the model.\n",
      "2023-05-08 19:43:49 [info     ] Loading training and validation set.\n",
      "2023-05-08 19:43:49 [info     ] Preparing the data.\n",
      "2023-05-08 19:43:49 [debug    ] Processes 8421/8421 (100%) of train data\n",
      "2023-05-08 19:43:49 [info     ] Training on device: cpu.\n",
      "2023-05-08 19:43:49 [info     ] Using CUSTOM loss\n",
      "2023-05-08 19:43:49 [info     ] Using ADAM optimizer\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, datasets=train_set,\n",
    "                  epochs=10, batch_size=2,\n",
    "                  is_parallel=False,\n",
    "                  custom_model=True,\n",
    "                  **config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-08 19:43:49 [info     ] Start training..\n",
      "2023-05-08 19:43:49 [info     ] ------------------------------ EPOCH 1 / 10 ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4211 [00:03<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got list",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Models/LLMtoSQL/llmtosql/trainer.py:332\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m    331\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m30\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m EPOCH \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m / \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m30\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 332\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    333\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclear()\n\u001B[1;32m    334\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalidate:\n",
      "File \u001B[0;32m~/Models/LLMtoSQL/llmtosql/trainer.py:244\u001B[0m, in \u001B[0;36mTrainer._train_one_epoch\u001B[0;34m(self, epoch)\u001B[0m\n\u001B[1;32m    242\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mmodule\u001B[38;5;241m.\u001B[39mloss(outputs, targets)\n\u001B[1;32m    243\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 244\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    245\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    246\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion(outputs, targets)\n",
      "File \u001B[0;32m~/Models/LLMtoSQL/llmtosql/model.py:127\u001B[0m, in \u001B[0;36mWikiSQLModel.loss\u001B[0;34m(self, outputs, targets)\u001B[0m\n\u001B[1;32m    125\u001B[0m     loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion(cond_out, cond_lab)\n\u001B[1;32m    126\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m idx \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 127\u001B[0m     loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion(cond_out, torch\u001B[38;5;241m.\u001B[39mtranspose(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcond_lab\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m idx \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[1;32m    129\u001B[0m     loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion(torch\u001B[38;5;241m.\u001B[39mtranspose(cond_out, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m ),\n\u001B[1;32m    130\u001B[0m                            torch\u001B[38;5;241m.\u001B[39mtranspose(torch\u001B[38;5;241m.\u001B[39mstack(cond_lab), \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m))\n",
      "\u001B[0;31mTypeError\u001B[0m: expected Tensor as element 0 in argument 0, but got list"
     ]
    }
   ],
   "source": [
    "trainer.fit()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
